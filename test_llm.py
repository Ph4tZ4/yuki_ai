#!/usr/bin/env python3
"""
Test script for LLM integration with Yuki AI
"""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_llm_basic():
    """Test basic LLM functionality"""
    try:
        from core.llm_engine import llm_engine
        
        print("ü§ñ Testing LLM Integration")
        print("=" * 30)
        
        # Check if any LLM service is available
        if not llm_engine.is_available():
            print("‚ùå No LLM service is available")
            print("LLM will use fallback responses")
        
        print(f"üì¶ Using model: {llm_engine.model_name}")
        print(f"üîß LLM enabled: {llm_engine.enable_llm}")
        
        # Test simple conversation
        test_questions = [
            "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏¢‡∏π‡∏Å‡∏¥",
            "‡∏Ñ‡∏∏‡∏ì‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á?",
            "‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏´‡∏ô‡πà‡∏≠‡∏¢",
            "What is artificial intelligence?",
            "‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏≠‡∏£‡πà‡∏≠‡∏¢‡πÜ ‡∏´‡∏ô‡πà‡∏≠‡∏¢"
        ]
        
        for question in test_questions:
            print(f"\nü§î User: {question}")
            response = llm_engine.generate_response(question)
            print(f"ü§ñ Yuki: {response}")
            print("-" * 50)
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

def test_llm_with_context():
    """Test LLM with context"""
    try:
        from core.llm_engine import llm_engine
        
        print("\nüß† Testing LLM with Context")
        print("=" * 30)
        
        # Test conversation with context
        context = "You are helping a user plan a trip to Thailand"
        
        questions = [
            "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û",
            "‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏≠‡∏á",
            "‡∏Ñ‡∏ß‡∏£‡πÑ‡∏õ‡∏ä‡πà‡∏ß‡∏á‡πÑ‡∏´‡∏ô‡∏î‡∏µ"
        ]
        
        for question in questions:
            print(f"\nü§î User: {question}")
            response = llm_engine.generate_response(question, context)
            print(f"ü§ñ Yuki: {response}")
            print("-" * 50)
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

def main():
    """Main test function"""
    print("üß™ Yuki AI - LLM Integration Test")
    print("=" * 40)
    
    # Test basic functionality
    if test_llm_basic():
        print("\n‚úÖ Basic LLM test passed!")
    else:
        print("\n‚ùå Basic LLM test failed!")
        return
    
    # Test with context
    if test_llm_with_context():
        print("\n‚úÖ Context LLM test passed!")
    else:
        print("\n‚ùå Context LLM test failed!")
    
    print("\nüéâ All tests completed!")

if __name__ == "__main__":
    main()
